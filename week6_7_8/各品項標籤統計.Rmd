---
title: "各品項標籤統計"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r cars}
#清空所有記憶體
rm(list=ls(all.names = TRUE))
library(NLP)
library(tm) #文字探勘
library(jiebaRD)
library(jiebaR) #中文斷詞 
library(RColorBrewer)
library(wordcloud) #產生文字雲

library(tidytext)
library(dplyr)
library(ggplot2)
library(stringr)
library(tidyr)
#install.packages("extrafont");
# ggplot font size
#亂碼方框
library(extrafont)
loadfonts()
par(family="STKaiti")


#因為mac編碼問題讀入csv檔案會較麻煩，所以讀入XML

library(readxl)
臉部防曬 <- read_excel("~/Desktop/各品項爬蟲資料/臉部防曬.xlsx")
保養面膜 <- read_excel("~/Desktop/各品項爬蟲資料/保養面膜.xlsx")
篩紅爬蟲<- read_excel("~/Desktop/各品項爬蟲資料/篩紅爬蟲.xlsx")
唇膏爬蟲 <- read_excel("~/Desktop/各品項爬蟲資料/唇膏爬蟲.xlsx")
睫毛爬蟲 <- read_excel("~/Desktop/各品項爬蟲資料/睫毛爬蟲.xlsx")

#讀取label
sunscreen_label <-臉部防曬$label
mask_label <-保養面膜$label
cheek_label <-篩紅爬蟲$label
Lip_label <-唇膏爬蟲$label
mascara_label<-睫毛爬蟲$label

#以頓號隔開
sunscreen_label <-strsplit(sunscreen_label,split="、",fixed=T)  
mask_label <-strsplit(mask_label,split="、",fixed=T)  
cheek_label <-strsplit(cheek_label,split="、",fixed=T)  
Lip_label <-strsplit(Lip_label,split="、",fixed=T)  
mascara_label <-strsplit(mascara_label,split="、",fixed=T)  

#sunscreen_label＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝
# EXAMPLE A (臉部防曬 各label統計) (畫長條圖)

tag_count <- 臉部防曬$label %>% strsplit('、') %>% unlist %>% table 
tag_count <- as.data.frame(tag_count)

colnames(tag_count)<- c("x", "freq")

ggplot(tag_count, aes(x = reorder(x,-freq), y = freq)) + 
  geom_bar(stat = "identity", fill='lightblue') + 
  labs(x='tag',title='臉部防曬 各label統計') + 
  theme(panel.background = element_blank(),
        axis.title = element_text(color = '#2d2d2d'),
        axis.text.x = element_text(hjust = 1, size=8,angle = 45),
        axis.text.y = element_text(hjust = 1, size=10),
        strip.text.x = element_text(color='#2d2d2d',face='bold',size=10),
        plot.title = element_text(hjust=0.5,face='bold',size=15),
        text=element_text(size=16, family="Noto Sans CJK TC"))

# 切割
tag_count <- 臉部防曬$label %>% strsplit('、')

# 轉成詞庫
s.corpus <- Corpus(VectorSource(tag_count))

#詞頻矩陣
s.dtm <- DocumentTermMatrix(s.corpus ,control = list(wordLengths = c(1, Inf),
                                                     weighting = weightTfIdf))

#移除 過度 少的
s.dtm = removeSparseTerms(s.dtm, 0.8)
rowTotals <- apply(s.dtm , 2, sum)
rowTotals <-sort(rowTotals,decreasing = TRUE)

#前10關鍵字
names(rowTotals[1:10])

# tfidf data frame
df <- data.frame()
df <- as.data.frame(as.matrix(s.dtm), stringsAsFactors=False)


#文字雲＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝

freqFrame = as.data.frame(table(unlist(sunscreen_label)))

mixseg = worker()
jieba_tokenizer=function(d){
  unlist(segment(d[[1]],mixseg))
}
par(family=("Heiti TC Light"))

wordcloud(freqFrame$Var1,freqFrame$Freq,
          scale=c(5,0.5),min.freq=10,max.words=50,
          random.order=FALSE, random.color=TRUE, 
          rot.per=0, colors=brewer.pal(8, "Dark2"),
          ordered.colors=FALSE,use.r.layout=FALSE,
          fixed.asp=TRUE)

#階層式集群分析作為資料探索的方法，利用dist()函數計算距離後，再用hclust()函數進行分析及繪圖。

set.seed(500)
#Distance <- dist(df, method = 'euclidean')

#hclust(Distance, method = 'complete') %>% plot()

#建模將以這3、4兩種群數分別討論。

#1. 分為4群
#用kmeans()演算法，決定分群數目後、進行分群後
# Implement Kmeans Algorithm:

set.seed(500) # remove the random effect
K <- kmeans(df,6)

ClusterResult <- cbind(
  df,
  K$cluster
) %>% as.data.frame()

colnames(ClusterResult)[ncol(ClusterResult)] <- 'Cluster'

table(ClusterResult$Cluster)

#納入主成份分析
library(ggfortify)
set.seed(500)
autoplot(kmeans(df[,1:5], 6), data  = df) 






#mask_label＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝
tag_count <- 保養面膜$label %>% strsplit('、') %>% unlist %>% table 
tag_count <- as.data.frame(tag_count)

colnames(tag_count)<- c("x", "freq")

ggplot(tag_count, aes(x = reorder(x,-freq), y = freq)) + 
  geom_bar(stat = "identity", fill='lightblue') + 
  labs(x='tag',title='保養面膜 各label統計') + 
  theme(panel.background = element_blank(),
        axis.title = element_text(color = '#2d2d2d'),
        axis.text.x = element_text(hjust = 1, size=8,angle = 45),
        axis.text.y = element_text(hjust = 1, size=10),
        strip.text.x = element_text(color='#2d2d2d',face='bold',size=10),
        plot.title = element_text(hjust=0.5,face='bold',size=15),
        text=element_text(size=16, family="Noto Sans CJK TC"))

# 切割
tag_count <- 保養面膜$label %>% strsplit('、')

# 轉成詞庫
s.corpus <- Corpus(VectorSource(tag_count))

#詞頻矩陣
s.dtm <- DocumentTermMatrix(s.corpus ,control = list(wordLengths = c(1, Inf),
                                                     weighting = weightTfIdf))

#移除 過度 少的
s.dtm = removeSparseTerms(s.dtm, 0.8)
rowTotals <- apply(s.dtm , 2, sum)
rowTotals <-sort(rowTotals,decreasing = TRUE)

#前10關鍵字
names(rowTotals[1:10])

# tfidf data frame
df <- data.frame()
df <- as.data.frame(as.matrix(s.dtm), stringsAsFactors=False)

#文字雲

freqFrame = as.data.frame(table(unlist(mask_label)))

mixseg = worker()
jieba_tokenizer=function(d){
  unlist(segment(d[[1]],mixseg))
}
par(family=("Heiti TC Light"))

wordcloud(freqFrame$Var1,freqFrame$Freq,
          scale=c(5,0.5),min.freq=10,max.words=50,
          random.order=FALSE, random.color=TRUE, 
          rot.per=0, colors=brewer.pal(8, "Dark2"),
          ordered.colors=FALSE,use.r.layout=FALSE,
          fixed.asp=TRUE)

#階層式集群分析作為資料探索的方法，利用dist()函數計算距離後，再用hclust()函數進行分析及繪圖。

set.seed(500)
Distance <- dist(df, method = 'euclidean')

hclust(Distance, method = 'complete') %>% plot()

#建模將以這3、4兩種群數分別討論。

#1. 分為4群
#用kmeans()演算法，決定分群數目後、進行分群後
# Implement Kmeans Algorithm:

set.seed(500) # remove the random effect
K <- kmeans(df,5)

ClusterResult <- cbind(
  df,
  K$cluster
) %>% as.data.frame()

colnames(ClusterResult)[ncol(ClusterResult)] <- 'Cluster'

table(ClusterResult$Cluster)

#納入主成份分析
library(ggfortify)
set.seed(500)
autoplot(kmeans(df[,1:5], 5), data  = df) 



#cheek_label＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝

tag_count <- 篩紅爬蟲$label %>% strsplit('、') %>% unlist %>% table 
tag_count <- as.data.frame(tag_count)

colnames(tag_count)<- c("x", "freq")

ggplot(tag_count, aes(x = reorder(x,-freq), y = freq)) + 
  geom_bar(stat = "identity", fill='lightblue') + 
  labs(x='tag',title='篩紅爬蟲 各label統計') + 
  theme(panel.background = element_blank(),
        axis.title = element_text(color = '#2d2d2d'),
        axis.text.x = element_text(hjust = 1, size=8,angle = 45),
        axis.text.y = element_text(hjust = 1, size=10),
        strip.text.x = element_text(color='#2d2d2d',face='bold',size=10),
        plot.title = element_text(hjust=0.5,face='bold',size=15),
        text=element_text(size=16, family="Noto Sans CJK TC"))

# 切割
tag_count <- 篩紅爬蟲$label %>% strsplit('、')

# 轉成詞庫
s.corpus <- Corpus(VectorSource(tag_count))

#詞頻矩陣
s.dtm <- DocumentTermMatrix(s.corpus ,control = list(wordLengths = c(1, Inf),
                                                     weighting = weightTfIdf))

#移除 過度 少的
s.dtm = removeSparseTerms(s.dtm, 0.8)
rowTotals <- apply(s.dtm , 2, sum)
rowTotals <-sort(rowTotals,decreasing = TRUE)

#前10關鍵字
names(rowTotals[1:10])

# tfidf data frame
df <- data.frame()
df <- as.data.frame(as.matrix(s.dtm), stringsAsFactors=False)

#文字雲＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝

freqFrame = as.data.frame(table(unlist(cheek_label)))

mixseg = worker()
jieba_tokenizer=function(d){
  unlist(segment(d[[1]],mixseg))
}
par(family=("Heiti TC Light"))

wordcloud(freqFrame$Var1,freqFrame$Freq,
          scale=c(5,0.5),min.freq=10,max.words=50,
          random.order=FALSE, random.color=TRUE, 
          rot.per=0, colors=brewer.pal(8, "Dark2"),
          ordered.colors=FALSE,use.r.layout=FALSE,
          fixed.asp=TRUE)
#階層式集群分析作為資料探索的方法，利用dist()函數計算距離後，再用hclust()函數進行分析及繪圖。

set.seed(500)
Distance <- dist(df, method = 'euclidean')

hclust(Distance, method = 'complete') %>% plot()

#建模將以這3、4兩種群數分別討論。

#1. 分為4群
#用kmeans()演算法，決定分群數目後、進行分群後
# Implement Kmeans Algorithm:

set.seed(500) # remove the random effect
K <- kmeans(df,5)

ClusterResult <- cbind(
  df,
  K$cluster
) %>% as.data.frame()

colnames(ClusterResult)[ncol(ClusterResult)] <- 'Cluster'

table(ClusterResult$Cluster)

#納入主成份分析
library(ggfortify)
set.seed(500)
autoplot(kmeans(df[,1:5], 5), data  = df) 


#Lip_label＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝

tag_count <- 唇膏爬蟲$label %>% strsplit('、') %>% unlist %>% table 
tag_count <- as.data.frame(tag_count)

colnames(tag_count)<- c("x", "freq")

ggplot(tag_count, aes(x = reorder(x,-freq), y = freq)) + 
  geom_bar(stat = "identity", fill='lightblue') + 
  labs(x='tag',title='唇膏爬蟲 各label統計') + 
  theme(panel.background = element_blank(),
        axis.title = element_text(color = '#2d2d2d'),
        axis.text.x = element_text(hjust = 1, size=8,angle = 45),
        axis.text.y = element_text(hjust = 1, size=10),
        strip.text.x = element_text(color='#2d2d2d',face='bold',size=10),
        plot.title = element_text(hjust=0.5,face='bold',size=15),
        text=element_text(size=16, family="Noto Sans CJK TC"))

# 切割
tag_count <- 唇膏爬蟲$label %>% strsplit('、')

# 轉成詞庫
s.corpus <- Corpus(VectorSource(tag_count))

#詞頻矩陣
s.dtm <- DocumentTermMatrix(s.corpus ,control = list(wordLengths = c(1, Inf),
                                                     weighting = weightTfIdf))

#移除 過度 少的
s.dtm = removeSparseTerms(s.dtm, 0.8)
rowTotals <- apply(s.dtm , 2, sum)
rowTotals <-sort(rowTotals,decreasing = TRUE)

#前10關鍵字
names(rowTotals[1:10])

# tfidf data frame
df <- data.frame()
df <- as.data.frame(as.matrix(s.dtm), stringsAsFactors=False)


#文字雲＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝

freqFrame = as.data.frame(table(unlist(Lip_label)))

mixseg = worker()
jieba_tokenizer=function(d){
  unlist(segment(d[[1]],mixseg))
}
par(family=("Heiti TC Light"))

wordcloud(freqFrame$Var1,freqFrame$Freq,
          scale=c(5,0.5),min.freq=10,max.words=50,
          random.order=FALSE, random.color=TRUE, 
          rot.per=0, colors=brewer.pal(8, "Dark2"),
          ordered.colors=FALSE,use.r.layout=FALSE,
          fixed.asp=TRUE)

#階層式集群分析作為資料探索的方法，利用dist()函數計算距離後，再用hclust()函數進行分析及繪圖。

set.seed(500)
Distance <- dist(df, method = 'euclidean')

hclust(Distance, method = 'complete') %>% plot()

#建模將以這3、4兩種群數分別討論。

#1. 分為5群
#用kmeans()演算法，決定分群數目後、進行分群後
# Implement Kmeans Algorithm:

set.seed(500) # remove the random effect
K <- kmeans(df,5)

ClusterResult <- cbind(
  df,
  K$cluster
) %>% as.data.frame()

colnames(ClusterResult)[ncol(ClusterResult)] <- 'Cluster'

table(ClusterResult$Cluster)

#納入主成份分析
library(ggfortify)
set.seed(500)
autoplot(kmeans(df[,1:5], 5), data  = df) 

#mascara_label＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝

tag_count <- 睫毛爬蟲$label %>% strsplit('、') %>% unlist %>% table 
tag_count <- as.data.frame(tag_count)

colnames(tag_count)<- c("x", "freq")

ggplot(tag_count, aes(x = reorder(x,-freq), y = freq)) + 
  geom_bar(stat = "identity", fill='lightblue') + 
  labs(x='tag',title='睫毛爬蟲 各label統計') + 
  theme(panel.background = element_blank(),
        axis.title = element_text(color = '#2d2d2d'),
        axis.text.x = element_text(hjust = 1, size=8,angle = 45),
        axis.text.y = element_text(hjust = 1, size=10),
        strip.text.x = element_text(color='#2d2d2d',face='bold',size=10),
        plot.title = element_text(hjust=0.5,face='bold',size=15),
        text=element_text(size=16, family="Noto Sans CJK TC"))

# 切割
tag_count <- 睫毛爬蟲$label %>% strsplit('、')

# 轉成詞庫
s.corpus <- Corpus(VectorSource(tag_count))

#詞頻矩陣
s.dtm <- DocumentTermMatrix(s.corpus ,control = list(wordLengths = c(1, Inf),
                                                     weighting = weightTfIdf))

#移除 過度 少的
s.dtm = removeSparseTerms(s.dtm, 0.8)
rowTotals <- apply(s.dtm , 2, sum)
rowTotals <-sort(rowTotals,decreasing = TRUE)

#前10關鍵字
names(rowTotals[1:10])

# tfidf data frame
df <- data.frame()
df <- as.data.frame(as.matrix(s.dtm), stringsAsFactors=False)

#文字雲＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝

freqFrame = as.data.frame(table(unlist(mascara_label)))

mixseg = worker()
jieba_tokenizer=function(d){
  unlist(segment(d[[1]],mixseg))
}
par(family=("Heiti TC Light"))

wordcloud(freqFrame$Var1,freqFrame$Freq,
          scale=c(5,0.5),min.freq=10,max.words=50,
          random.order=FALSE, random.color=TRUE, 
          rot.per=0, colors=brewer.pal(8, "Dark2"),
          ordered.colors=FALSE,use.r.layout=FALSE,
          fixed.asp=TRUE)
#階層式集群分析作為資料探索的方法，利用dist()函數計算距離後，再用hclust()函數進行分析及繪圖。

set.seed(500)
Distance <- dist(df, method = 'euclidean')

hclust(Distance, method = 'complete') %>% plot()

#建模將以這3、4兩種群數分別討論。

#1. 分為4群
#用kmeans()演算法，決定分群數目後、進行分群後
# Implement Kmeans Algorithm:

set.seed(500) # remove the random effect
K <- kmeans(df,5)

ClusterResult <- cbind(
  df,
  K$cluster
) %>% as.data.frame()

colnames(ClusterResult)[ncol(ClusterResult)] <- 'Cluster'

table(ClusterResult$Cluster)

#納入主成份分析
library(ggfortify)
set.seed(500)
autoplot(kmeans(df[,1:5], 5), data  = df) 
#＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝


```

