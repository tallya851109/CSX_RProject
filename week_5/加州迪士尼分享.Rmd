---
title: "加州迪士尼網路旅遊分享"
output: html_document
---
因為想去加州迪士尼玩，所以這次以google“加州迪士尼”搜尋到的首頁旅遊分享作為這次分析的文本

選了找了9篇，看看其內容有什麼重要資訊

Load required libraries.
```{r}
library(NLP)
library(tm)
library(ggplot2)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
library(RColorBrewer)
library(wordcloud)
```

載入資料
```{r}
#setwd("~/Desktop/CSX_RProject/week_5/迪士尼攻略txt檔")
rawData = readtext("*.txt")
docs = Corpus(VectorSource(rawData$text))
length(docs)
docs[[1]]
```

data clean

```{r}
toSpace <- content_transformer(function(x, pattern) {
  return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")

#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) {
  return (gsub(pattern, " ", x))
}
)
docs <- tm_map(docs, toSpace, "個")
docs <- tm_map(docs, toSpace, "小")
docs <- tm_map(docs, toSpace, "下")
docs <- tm_map(docs, toSpace, "不")
docs <- tm_map(docs, toSpace, "中")
docs <- tm_map(docs, toSpace, "之")
docs <- tm_map(docs, toSpace, "再")
docs <- tm_map(docs, toSpace, "了")
docs <- tm_map(docs, toSpace, "們")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "在")
docs <- tm_map(docs, toSpace, "像是")
docs <- tm_map(docs, toSpace, "先祝")
docs <- tm_map(docs, toSpace, "內")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "出")
docs <- tm_map(docs, toSpace, "到")
docs <- tm_map(docs, toSpace, "去")
docs <- tm_map(docs, toSpace, "又")
docs <- tm_map(docs, toSpace, "和")
docs <- tm_map(docs, toSpace, "就")
docs <- tm_map(docs, toSpace, "看")
docs <- tm_map(docs, toSpace, "很")
docs <- tm_map(docs, toSpace, "我")
docs <- tm_map(docs, toSpace, "來")
docs <- tm_map(docs, toSpace, "著")
docs <- tm_map(docs, toSpace, "為")

```


結巴字典
```{r}
mixseg = worker()
jieba_tokenizer=function(d){
  unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))

```
words cut
```{r}
#keyword = read.csv ("keywords.csv")
#mixseg = worker()
#keys = as.matrix(keyword)
#new_user_word(mixseg, keys)

jieba_tokenizer = function(d){
  unlist(segment(d[[1]], mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))

d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)
print( tf <- as.matrix(tdm) )
DF <- tidy(tf)
```
Take a look at the summary .
```{r}
docs
```

Analyse how frequently terms appear by summing the content of all terms .
```{r}
freq=rowSums(as.matrix(tdm))
head(freq,10)

tail(freq,10)
```
Plot those frequencies ordered.
```{r}
plot(sort(freq, decreasing = T),col="red",main="Word TF-IDF frequencies", xlab="TF-IDF-based rank", ylab = "TF-IDF")
```

See the ten most frequent terms
```{r}
tail(sort(freq),n=10)
```

Show most frequent terms and their frequencies in a bar plot.

```{r}
high.freq=tail(sort(freq),n=9)
hfp.df=as.data.frame(sort(high.freq))
hfp.df$names <- rownames(hfp.df) 

ggplot(hfp.df, aes(reorder(names,high.freq), high.freq)) +
  geom_bar(stat="identity") + coord_flip() + 
  xlab("Terms") + ylab("Frequency") +
  ggtitle("Term frequencies")

```


docs.dfm <- dfm(docs, tolower = FALSE, remove = stopwords())
```{r}
tf <- as.matrix(tdm) 
tdm <- t(tf)
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=3)
library(ggplot2)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") +
  xlab("Terms") + ylab("Count") + coord_flip()
```


文字雲
```{r}
library("jiebaR")
Sys.setlocale(category = "LC_ALL", locale = "cht")


library(wordcloud)

par(family=("HeitiTC Light"))

mixseg = worker()
jieba_tokenizer=function(d){
  unlist(segment(d[[1]],mixseg))
}
par(family=("Heiti TC Light"))

wordcloud(freqFrame$Var1,freqFrame$Freq,
          scale=c(5,0.5),min.freq=10,max.words=50,
          random.order=FALSE, random.color=TRUE, 
          rot.per=0, colors=brewer.pal(8, "Dark2"),
          ordered.colors=FALSE,use.r.layout=FALSE,
          fixed.asp=TRUE)
```

Conclusion
透過這次分析, 我知道

