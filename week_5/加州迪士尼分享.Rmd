---
title: "加州迪士尼網路旅遊分享"
output: html_document
---
想分析熱門旅遊遊記常用到的詞彙與不同旅遊分享平台（部落格、pixnet、kkday...）用字的差異

此次先以google“加州迪士尼”搜尋到的旅遊分享作為這次分析的文本

未來想擴大範圍，大量分析、比較這些高點擊率的旅遊分享用字或分享了哪些資訊～

Load required libraries.
```{r}
library(NLP)
library(tm)
library(ggplot2)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
library(RColorBrewer)
library(wordcloud)
```

載入資料
```{r}
setwd("~/Desktop/CSX_RProject/week_5/加州迪士尼分享txt檔")
rawData = readtext("*.txt")
docs = Corpus(VectorSource(rawData$text))
length(docs)
docs[[1]]
```

data clean

```{r}
toSpace <- content_transformer(function(x, pattern) {
  return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")

#移除可能有問題的符號
toSpace <- content_transformer(function(x, pattern) {
  return (gsub(pattern, " ", x))
}
)
docs <- tm_map(docs, toSpace, "個")
docs <- tm_map(docs, toSpace, "小")
docs <- tm_map(docs, toSpace, "下")
docs <- tm_map(docs, toSpace, "不")
docs <- tm_map(docs, toSpace, "中")
docs <- tm_map(docs, toSpace, "之")
docs <- tm_map(docs, toSpace, "再")
docs <- tm_map(docs, toSpace, "了")
docs <- tm_map(docs, toSpace, "們")
docs <- tm_map(docs, toSpace, "是")
docs <- tm_map(docs, toSpace, "在")
docs <- tm_map(docs, toSpace, "先祝")
docs <- tm_map(docs, toSpace, "內")
docs <- tm_map(docs, toSpace, "的")
docs <- tm_map(docs, toSpace, "出")
docs <- tm_map(docs, toSpace, "到")
docs <- tm_map(docs, toSpace, "去")
docs <- tm_map(docs, toSpace, "又")
docs <- tm_map(docs, toSpace, "和")
docs <- tm_map(docs, toSpace, "就")
docs <- tm_map(docs, toSpace, "看")
docs <- tm_map(docs, toSpace, "很")
docs <- tm_map(docs, toSpace, "來")
docs <- tm_map(docs, toSpace, "著")
docs <- tm_map(docs, toSpace, "為")

```


結巴字典
```{r}
mixseg = worker()
jieba_tokenizer=function(d){
  unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))

```
words cut
```{r}
#keyword = read.csv ("keywords.csv")
#mixseg = worker()
#keys = as.matrix(keyword)
#new_user_word(mixseg, keys)

jieba_tokenizer = function(d){
  unlist(segment(d[[1]], mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))

d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)
print( tf <- as.matrix(tdm) )
DF <- tidy(tf)
```
Take a look at the summary .
```{r}
docs
```

Analyse how frequently terms appear by summing the content of all terms .
```{r}
freq=rowSums(as.matrix(tdm))
head(freq,10)

tail(freq,10)
```
Plot those frequencies ordered.
```{r}
plot(sort(freq, decreasing = T),col="red",main="Word TF-IDF frequencies", xlab="TF-IDF-based rank", ylab = "TF-IDF")
```

See the ten most frequent terms
```{r}
tail(sort(freq),n=10)
```

Show most frequent terms and their frequencies in a bar plot.

```{r}
high.freq=tail(sort(freq),n=9)
hfp.df=as.data.frame(sort(high.freq))
hfp.df$names <- rownames(hfp.df) 

ggplot(hfp.df, aes(reorder(names,high.freq), high.freq)) +
  geom_bar(stat="identity") + coord_flip() + 
  xlab("Terms") + ylab("Frequency") +
  ggtitle("Term frequencies")

```


docs.dfm <- dfm(docs, tolower = FALSE, remove = stopwords())
```{r}
tf <- as.matrix(tdm) 
tdm <- t(tf)
termFrequency <- rowSums(as.matrix(tdm))
termFrequency <- subset(termFrequency, termFrequency>=3)
library(ggplot2)
df <- data.frame(term=names(termFrequency), freq=termFrequency)
ggplot(df, aes(x=term, y=freq)) + geom_bar(stat="identity") +
  xlab("Terms") + ylab("Count") + coord_flip()
```


文字雲
```{r}
library("jiebaR")
Sys.setlocale(category = "LC_ALL", locale = "cht")


library(wordcloud)

par(family=("HeitiTC Light"))

mixseg = worker()
jieba_tokenizer=function(d){
  unlist(segment(d[[1]],mixseg))
}
par(family=("Heiti TC Light"))

wordcloud(freqFrame$Var1,freqFrame$Freq,
          scale=c(5,0.5),min.freq=10,max.words=50,
          random.order=FALSE, random.color=TRUE, 
          rot.per=0, colors=brewer.pal(8, "Dark2"),
          ordered.colors=FALSE,use.r.layout=FALSE,
          fixed.asp=TRUE)
```

Conclusion
因為此次是迪士尼旅遊分享，所以主題明確，每一篇也都會被提到
可以發現旅遊中的活動名稱很多都有被顯示出來
而“可以”、“有”、“能”、“你”、“我”是很大量會用到的詞
因為此次文本不夠多，所以能觀察到的事情就有限
之後想多嘗試，可能有其他發現
